--importing required functionalities

import itertools
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
import pandas as pd
import numpy as np
import matplotlib.ticker as ticker
from sklearn import preprocessing
%matplotlib inline

//importing data

df=pd.read_csv("teleCust1000t.csv")
df.describe()

// to see number in different classes
df['custcat'].value_counts

//datavisualization
df.hist(column='income', bins=50)

//defining feature sets
df.columns----names of all columns

//To use scikit-learn library, we have to convert the Pandas data frame to a Numpy array:

(X-axis)X = df[['region', 'tenure','age', 'marital', 'address', 'income', 'ed', 'employ','retire', 'gender', 'reside']] .values  #.astype(float)
X[0:5]

(Y-axis) y = df['custcat'].values
y[0:5]

//now we need to normalize the data
Data Standardization give data zero mean and unit variance, it is good practice, especially for algorithms such as KNN which is based on distance of cases:

X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))
X[0:5]

//train test split

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

//classification

--import library
from sklearn.neighbors import KNeighborsClassifier

--training for k=4

k = 4
#Train Model and Predict  
neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
neigh

--predicting
yhat = neigh.predict(X_test)
yhat[0:5]

--accuracy evaluation

from sklearn import metrics
print("Train set Accuracy: ", metrics.accuracy_score(y_train, neigh.predict(X_train)))
print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat))


--for k=6

k=6
neigh6=KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
yhat6=neigh.predict(X-test)
from sklearn import metrics 
print("Train Set Accuracy:"metrics.accuracy_score(y_train,neigh6.predict(X_train)))
print("Test set Accuracy:",metrics.accuracy_score(y_test,yhat6)


//What about other K?
K in KNN, is the number of nearest neighbors to examine. It is supposed to be specified by the User. So, how can we choose right value for K? The general solution is to reserve a part of your data for testing the accuracy of the model. Then chose k =1, use the training part for modeling, and calculate the accuracy of prediction using all samples in your test set. Repeat this process, increasing the k, and see which k is the best for your model.

We can calculate the accuracy of KNN for different Ks.
calculates accuracy for all values of k
Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))
ConfustionMx = [];
for n in range(1,Ks):
    
    #Train Model and Predict  
    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
    yhat=neigh.predict(X_test)
    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)

    
    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])

mean_acc

--model accuracy for different number of neighbors

plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.legend(('Accuracy ', '+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Nabors (K)')
plt.tight_layout()
plt.show()

--best accuracy

print( "The best accuracy was with", mean_acc.max(), "with k=", mean_acc.argmax()+1) 